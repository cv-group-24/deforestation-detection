{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import all dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "# TODO: Handle relative paths correctly so we can all run it independantly and add to the readme where the dataset needs to be stored\n",
    "dataset_path = r\"C:\\Users\\chris\\Desktop\\University\\Code\\ComputerVision\\ForestNetDataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset into memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label mapping: {'Grassland shrubland': 0, 'Other': 1, 'Plantation': 2, 'Smallholder agriculture': 3}\n"
     ]
    }
   ],
   "source": [
    "# Join the directory with each CSV filename.\n",
    "test_path = os.path.join(dataset_path, \"test.csv\")\n",
    "train_path = os.path.join(dataset_path, \"train.csv\")\n",
    "validation_path = os.path.join(dataset_path, \"val.csv\")\n",
    "\n",
    "# Read the CSV files into pandas DataFrames.\n",
    "test_df = pd.read_csv(test_path)\n",
    "train_df = pd.read_csv(train_path)\n",
    "val_df = pd.read_csv(validation_path)\n",
    "\n",
    "# Create a mapping from the string labels to integers based on the training data.\n",
    "labels = sorted(train_df[\"merged_label\"].unique())\n",
    "label_to_index = {label: idx for idx, label in enumerate(labels)}\n",
    "print(\"Label mapping:\", label_to_index)\n",
    "\n",
    "\n",
    "# FOR MODEL DEVELOPMENT JUST USE THE FIRST 128 SAMPLES FROM THE TRAINING SET\n",
    "train_df = train_df.head(128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define DataLoaders for the training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This class implements the function __getitem__ which means it can be passed into the DataLoader class from pytorch \n",
    "# which makes the batch processing much more seamless.\n",
    "class ForestNetDataset(Dataset):\n",
    "    def __init__(self, df, dataset_path, transform=None, label_map=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df (pd.DataFrame): DataFrame containing the image paths and labels.\n",
    "            dataset_path (str): The base directory for the images.\n",
    "            transform (callable, optional): A function/transform to apply to the images.\n",
    "            label_map (dict, optional): Mapping from label names to integers.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.dataset_path = dataset_path\n",
    "        self.transform = transform\n",
    "        self.label_map = label_map\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get the row for this index\n",
    "        row = self.df.iloc[idx]\n",
    "        # Build the full image path\n",
    "        image_rel_path = row[\"example_path\"] + \"/images/visible/composite.png\"\n",
    "        image_path = os.path.join(self.dataset_path, image_rel_path)\n",
    "        # Open the image (convert to RGB in case it's grayscale)\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        # Get the label and convert it to an integer using the provided map\n",
    "        label = row[\"merged_label\"]\n",
    "        if self.label_map is not None:\n",
    "            label = self.label_map[label]\n",
    "        return image, label\n",
    "\n",
    "# --- Image Transforms ---\n",
    "# Resize images to 224x224, convert them to tensors, and normalize.\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "\n",
    "    # TODO: Look into calculating these values for our dataset. It probably has a lot more green than other\n",
    "    # datasets.\n",
    "    # These normalization values are typical for natural images.\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# --- Create Datasets and DataLoaders ---\n",
    "train_dataset = ForestNetDataset(train_df, dataset_path, transform=transform, label_map=label_to_index)\n",
    "test_dataset = ForestNetDataset(test_df, dataset_path, transform=transform, label_map=label_to_index)\n",
    "\n",
    "batch_size = 1\n",
    "# TO DO: Experiment with different num_workers (I don't know what this does)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-1.8439, -1.8439, -1.7925,  ..., -1.9124, -1.9295, -1.9295],\n",
      "         [-1.8782, -1.8782, -1.7925,  ..., -1.9124, -1.9467, -1.9467],\n",
      "         [-1.9124, -1.8782, -1.8097,  ..., -1.9467, -1.9638, -1.9638],\n",
      "         ...,\n",
      "         [-1.8953, -1.9295, -1.9124,  ..., -1.8782, -1.8782, -1.8953],\n",
      "         [-1.8953, -1.9295, -1.9124,  ..., -1.8953, -1.8439, -1.8610],\n",
      "         [-1.9124, -1.9295, -1.9124,  ..., -1.8953, -1.8268, -1.8439]],\n",
      "\n",
      "        [[-1.6681, -1.6506, -1.6155,  ..., -1.6331, -1.6681, -1.6681],\n",
      "         [-1.6856, -1.6681, -1.6155,  ..., -1.6506, -1.6856, -1.6856],\n",
      "         [-1.7031, -1.6681, -1.6331,  ..., -1.6856, -1.6856, -1.7031],\n",
      "         ...,\n",
      "         [-1.6155, -1.6506, -1.6331,  ..., -1.6331, -1.6331, -1.6506],\n",
      "         [-1.5980, -1.6506, -1.6331,  ..., -1.6506, -1.5980, -1.6155],\n",
      "         [-1.5980, -1.6331, -1.6331,  ..., -1.6331, -1.5805, -1.5805]],\n",
      "\n",
      "        [[-1.5953, -1.5953, -1.5779,  ..., -1.6302, -1.6650, -1.6650],\n",
      "         [-1.6127, -1.6127, -1.5604,  ..., -1.6476, -1.6824, -1.6824],\n",
      "         [-1.6476, -1.6127, -1.5604,  ..., -1.6824, -1.6999, -1.6824],\n",
      "         ...,\n",
      "         [-1.6650, -1.6650, -1.6127,  ..., -1.5779, -1.5953, -1.6127],\n",
      "         [-1.6302, -1.6476, -1.6127,  ..., -1.5430, -1.5604, -1.6127],\n",
      "         [-1.6302, -1.6476, -1.6302,  ..., -1.5604, -1.5430, -1.5779]]]), 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.__getitem__(121))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ChrisEnv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
